{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Is Warren Buffet's Portfolio Choice Optimal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "Warren Buffet is the world's most prominent value investor. We decided to analyze his portfolio to see if there were any improvements to be made. We loosely make use of the work of:\n",
    "- Michael Pinelis and David Ruppert (Machine learning portfolio allocation(2021))\n",
    "-  Shihao Gu, Bryan Kelly, and Dacheng Xiu (Empirical Asset Pricing via Machine\n",
    " Learning(2019))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import missingno as msno \n",
    "import pandas as pd\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler, PowerTransformer, QuantileTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import base64\n",
    "from IPython.display import Image, display\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BuffetPortfolioOverview = pd.read_csv(\"Warren Buffet Portfolio Overview.csv\")\n",
    "BuffetPortfolioOverview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Research Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mm(graph):\n",
    "    graphbytes = graph.encode(\"utf8\")\n",
    "    base64_bytes = base64.b64encode(graphbytes)\n",
    "    base64_string = base64_bytes.decode(\"ascii\")\n",
    "    display(Image(url=\"https://mermaid.ink/img/\" + base64_string))\n",
    "\n",
    "mm(\"\"\"\n",
    "graph TD\n",
    "    subgraph \"Data Processing\"\n",
    "        A[Raw Data] --> B[Missing Value Imputation]\n",
    "        B --> C[Normalization]\n",
    "    end\n",
    "\n",
    "    subgraph \"Feature Engineering\"\n",
    "        D[Features] --> E[Fundamental Features]\n",
    "        D --> F[Technical Features]\n",
    "        D --> G[Sentiment Features]\n",
    "    end\n",
    "    \n",
    "    E --> H[RF Model Training]\n",
    "    F --> H\n",
    "    G --> H\n",
    "\t\t\n",
    "    subgraph \"Training\"\n",
    "        H[RF Model Training] --> I[Hyperparameter Optimization]\n",
    "        I --> J[Optimal Model]\n",
    "    end\n",
    "\n",
    "    subgraph \"Prediction\"\n",
    "        K[Test Data] --> L[Return Ratio Prediction]\n",
    "    end\n",
    "\n",
    "    subgraph \"Investment Strategy\"\n",
    "        M[Investment Strategy] --> N[Stock Selection]\n",
    "        N --> O[Investment Portfolio Return & Std]\n",
    "    end\n",
    "\n",
    "    subgraph \"Evaluation\"\n",
    "        P[Evaluation Method] --> Q[Sharpe Ratio]\n",
    "    end\n",
    "\n",
    "    C --> D\n",
    "    J --> L\n",
    "    L --> N\n",
    "    O --> Q\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "financial_ratios = pd.read_csv(\"2014-2024 Buffet Portfolio - financial ratios.csv\")\n",
    "financial_ratios.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock = pd.read_csv(\"2014-2024 Buffet Portfolio - stock.csv\")\n",
    "stock.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = pd.read_csv(\"merged_financial_stock_macro_data.csv\")\n",
    "merged_data = merged_data.drop(merged_data.columns[0], axis=1)\n",
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Descriptive Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data['date'] = pd.to_datetime(merged_data['date'])\n",
    "\n",
    "# Basic statistical description\n",
    "statistical_description = merged_data.describe()\n",
    "\n",
    "# Plotting the trends of a few selected metrics over time\n",
    "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(14, 6))\n",
    "\n",
    "# Plotting RET\n",
    "sns.lineplot(x='date', y='RET', data=merged_data).set_title('Price Over Time')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recalculate the correlation matrix\n",
    "correlation_matrix = merged_data.select_dtypes(include=['number']).drop(columns = 'SHRCD_encoded').corr()\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(9, 7))\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(correlation_matrix, annot=False, fmt=\".1f\", linewidths=.5, cmap=\"coolwarm\")\n",
    "plt.title('Correlation Matrix Heatmap')\n",
    "plt.show()\n",
    "plt.savefig('Correlation_matrix_heatmap.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out non-numeric columns from merged_data\n",
    "numeric_data = merged_data.select_dtypes(include=[np.number])\n",
    "\n",
    "# Create a figure and axis object\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Set the figure height and width\n",
    "fig.set_figheight(35)\n",
    "fig.set_figwidth(30)\n",
    "\n",
    "# Plot histograms for the numeric columns in numeric_data\n",
    "numeric_data.hist(layout=(-1, 3), bins=np.linspace(-1,1,50), ax=ax)\n",
    "\n",
    "# Set the title for the figure\n",
    "fig.suptitle('Distribution of Numeric Variables Over Time')\n",
    "\n",
    "# Save the figure as 'numbers_over_time.png'\n",
    "plt.savefig('Distribution_of_Numeric_Variables_Over_Time.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3 = merged_data[[\"RET\"]]\n",
    "df_3 = df_3.rename(columns={'RET': 'StockReturn'})\n",
    "sns.histplot(data = df_3, x =\"StockReturn\", binwidth = 0.01, binrange = (df_3[\"StockReturn\"].min()+ 0.00000000001, df_3[\"StockReturn\"].max()- 0.01))\n",
    "plt.title('stock return')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the global size of the figure\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Calculate the maximum market capitalization\n",
    "max_size = merged_data['market_cap'].max()\n",
    "\n",
    "# Plot the histogram\n",
    "ax = sns.histplot(data=merged_data, x=\"market_cap\", binrange=(0, max_size))\n",
    "ax.set_title('Companies by Market Size')  # Set the title\n",
    "\n",
    "# Define the image size\n",
    "fig.set_figheight(15)\n",
    "fig.set_figwidth(15)\n",
    "\n",
    "# Show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming final_data is defined\n",
    "ret_data = merged_data[[\"RET\", \"TICKER\", \"date\"]].copy()  # Create a copy to work on to avoid affecting original data\n",
    "\n",
    "# Create equal weights safely using loc\n",
    "ret_data['eq_weights'] = 1 / ret_data.groupby('date')['TICKER'].transform('size')\n",
    "\n",
    "# Calculate monthly return of the equally weighted portfolio containing all stocks\n",
    "# Use loc to ensure the operation is directly on the DataFrame\n",
    "ret_data.loc[:, 'return_stock_ew'] = ret_data['RET'].astype(float) * ret_data['eq_weights']\n",
    "\n",
    "# Calculate the total returns for each date\n",
    "ret_data.loc[:, 'Returns'] = ret_data.groupby('date')['return_stock_ew'].transform('sum')\n",
    "\n",
    "# Drop duplicates and set index\n",
    "unique_ret_data = ret_data[['date', 'Returns']].drop_duplicates().set_index('date')\n",
    "\n",
    "# Plot the portfolio returns over time\n",
    "unique_ret_data.plot(grid=True, figsize=(15, 7))\n",
    "plt.title('Portfolio Monthly Returns Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Returns')\n",
    "plt.grid(True)\n",
    "\n",
    "# Save the graph\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureEngineeringDF = merged_data['value'].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Models and their training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the independent variables (X) and the dependent variable (y)\n",
    "features = merged_data.columns[~merged_data.columns.isin(['TICKER', 'RET'])].tolist()\n",
    "X = merged_data[features]\n",
    "y = merged_data['RET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View features\n",
    "# Create a Random Forest model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Get feature importances and sort them\n",
    "feature_importances = model.feature_importances_\n",
    "indices = np.argsort(feature_importances)[::-1]\n",
    "\n",
    "# Display feature importances\n",
    "print(\"Feature ranking:\")\n",
    "for f in range(X.shape[1]):\n",
    "    print(f\"{f + 1}. feature {indices[f]} ({feature_importances[indices[f]]}) - {features[indices[f]]}\")\n",
    "\n",
    "# You can select features based on their importance, for example, selecting the top N important features\n",
    "selected_features = indices[:10]  # Select the top 10 important features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Selected features:\", [features[i] for i in selected_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "# 2. Model creation and training\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# 3. Model evaluation\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared Score: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Feature Importance\n",
    "feature_importance = pd.DataFrame({'feature': X.columns, 'importance': rf_model.feature_importances_})\n",
    "feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(feature_importance['feature'][:10], feature_importance['importance'][:10])\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title('Top 10 Feature Importances')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection\n",
    "selector = SelectFromModel(RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "X_selected = selector.fit_transform(X, y)\n",
    "\n",
    "# Standardization\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_selected)\n",
    "\n",
    "# Time series cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Model parameters\n",
    "rf_params = {\n",
    "    'n_estimators': 1000,\n",
    "    'max_depth': 10,\n",
    "    'min_samples_split': 5,\n",
    "    'min_samples_leaf': 2,\n",
    "    'max_features': 'sqrt',\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "rf_model = RandomForestRegressor(**rf_params)\n",
    "\n",
    "# Use time series cross-validation\n",
    "r2_scores = []\n",
    "for train_index, test_index in tscv.split(X_scaled):\n",
    "    X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    rf_model.fit(X_train, y_train)\n",
    "    r2_scores.append(rf_model.score(X_test, y_test))\n",
    "\n",
    "print(f\"Average R2 score: {np.mean(r2_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.layer1 = nn.Linear(4, 32)\n",
    "        self.layer2 = nn.Linear(32, 16)\n",
    "        self.layer3 = nn.Linear(16, 8)\n",
    "        self.output = nn.Linear(8, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.relu(self.layer3(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "model = NeuralNetwork()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Dummy data\n",
    "inputs = torch.randn(10, 4)\n",
    "targets = torch.randn(10, 1)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Prescriptive Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
